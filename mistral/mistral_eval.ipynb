{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer,pipeline\n",
    "from datasets import load_dataset\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/qvr905\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:19<00:00,  9.97s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_id = \"./FineTune/mistral/mistral_code\"\n",
    "\n",
    "# Load Model with PEFT adapter\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  model_id,\n",
    "  load_in_4bit=True,\n",
    "  device_map=\"auto\",\n",
    "  cache_dir=\"./.cache/huggingface\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "#pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, query, context=None):\n",
    "    # Ensure that the model and input tensors are on the same device\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Tokenize inputs\n",
    "    inputs = tokenizer(query, context, return_tensors=\"pt\", add_special_tokens=True)\n",
    "    # Set pad token ID and padding side\n",
    "    tokenizer.padding_side = 'left'\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    \n",
    "    # Generate output\n",
    "    output = model.generate(input_ids=input_ids,max_new_tokens =256 ,temperature = 0.9,top_k=100,num_beams=20)\n",
    "    #output = model.generate(input_ids=input_ids,max_new_tokens =256)\n",
    "    \n",
    "    \n",
    "    # Decode and return response\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "from codebleu import calc_codebleu\n",
    "from bert_score import score as bert_score\n",
    "\n",
    "# Load evaluation metrics\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "bert_metric = evaluate.load(\"bertscore\")\n",
    "codebleu_metric = evaluate.load(\"k4black/codebleu\")\n",
    "\n",
    "def evaluate(predictions, references,model):    \n",
    "    # Compute BERTScore\n",
    "    #bert_score_precision, bert_score_recall, bert_score_f1= bert_metric.compute(predictions=predictions, references=references, lang='en', verbose=False)\n",
    "    bert_score_precision, bert_score_recall, bert_score_f1 = bert_score(predictions, references, lang=\"en\", verbose=False)\n",
    "    #bert_score = bert_score_result['bertscore'].item()\n",
    "    # Compute BLEU\n",
    "    bleu_score = bleu_metric.compute(predictions=predictions, references=references,smooth=True)['bleu']\n",
    "    \n",
    "    # Compute Rouge-L\n",
    "    rouge_score = rouge_metric.compute(predictions=predictions, references=references)['rougeL']\n",
    "    \n",
    "    codebleu = calc_codebleu(references, predictions, lang=\"python\", weights=(0.25, 0.25, 0.25, 0.25), tokenizer=None)\n",
    "    return bleu_score, rouge_score,codebleu, bert_score_precision, bert_score_recall, bert_score_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44663\n",
      "4963\n",
      "{'output': '```python\\ndef staircase(n): \\n    for i in range(1, n + 1): \\n        for j in range(1, n + 1): \\n            if (j <= (n - i)):   \\n                print(end=\"\\\\t\") \\n            else: \\n                print(\"#\\\\t\", end=\"\") \\n        print()\\n```', 'text': 'Write a Python function that prints out a staircase pattern. No problem, I\\'m on it! ```python\\ndef staircase(n): \\n    for i in range(1, n + 1): \\n        for j in range(1, n + 1): \\n            if (j <= (n - i)):   \\n                print(end=\"\\\\t\") \\n            else: \\n                print(\"#\\\\t\", end=\"\") \\n        print()\\n```', 'instruction': 'Write a Python function that prints out a staircase pattern.', 'input': ''}\n",
      "{'output': \"```python\\nimport xml.etree.ElementTree as ET\\n\\n# Let me run it for you!\\n\\ntree = ET.parse('file.xml')\\nroot = tree.getroot()\\nfor child in root:\\n    print(child.tag, child.attrib)\\n```\", 'text': \"Write Python code to read an XML file and parse it using ElementTree. Reading and parsing an XML file... ```python\\nimport xml.etree.ElementTree as ET\\n\\n# Let me run it for you!\\n\\ntree = ET.parse('file.xml')\\nroot = tree.getroot()\\nfor child in root:\\n    print(child.tag, child.attrib)\\n```\", 'instruction': 'Write Python code to read an XML file and parse it using ElementTree.', 'input': 'Reading and parsing an XML file...'}\n",
      "{'output': '```python\\ndef findCommon(list1,list2): \\n    common = [] \\n      \\n    # traverse through elements of list1 \\n    for x in list1: \\n          \\n        # traverse through elements of list2 \\n        for y in list2: \\n              \\n            # if common element is found append to list \\n            if x == y: \\n                common.append(x) \\n    return common\\n```', 'text': \"Write a Python script to take two list and output the common elements of the given lists. list1 = [1, 2, 3, 4, 5]\\nlist2 = [4, 5, 6, 7, 8] Sure, I'm up for the task! ```python\\ndef findCommon(list1,list2): \\n    common = [] \\n      \\n    # traverse through elements of list1 \\n    for x in list1: \\n          \\n        # traverse through elements of list2 \\n        for y in list2: \\n              \\n            # if common element is found append to list \\n            if x == y: \\n                common.append(x) \\n    return common\\n```\", 'instruction': 'Write a Python script to take two list and output the common elements of the given lists. list1 = [1, 2, 3, 4, 5]\\nlist2 = [4, 5, 6, 7, 8]', 'input': ''}\n",
      "{'output': '```python\\ndef find_first_unique_char_index(s):\\n    # create a dict to store already encountered characters\\n    encountered = {}\\n\\n    # loop through the string and check if character is unique\\n    for i in range(len(s)):\\n        c = s[i]\\n        if c not in encountered:\\n            encountered[c] = i\\n        else:\\n            encountered[c] = -1\\n\\n    # find the first occurrence of an unique character\\n    for i in range(len(s)):\\n        c = s[i]\\n        if encountered[c] == i:\\n            return i\\n\\n    return -1\\n```', 'text': 'Create a python program with the following goal: Given a string, find the index of the first unique character. Initiating now! Hold on tight! ```python\\ndef find_first_unique_char_index(s):\\n    # create a dict to store already encountered characters\\n    encountered = {}\\n\\n    # loop through the string and check if character is unique\\n    for i in range(len(s)):\\n        c = s[i]\\n        if c not in encountered:\\n            encountered[c] = i\\n        else:\\n            encountered[c] = -1\\n\\n    # find the first occurrence of an unique character\\n    for i in range(len(s)):\\n        c = s[i]\\n        if encountered[c] == i:\\n            return i\\n\\n    return -1\\n```', 'instruction': 'Create a python program with the following goal: Given a string, find the index of the first unique character.', 'input': ''}\n",
      "{'output': 'print(*lst) # 1 2 3 4', 'text': \"Suggest an elegant way to print out a list of numbers without using a loop in Python. lst = [1, 2, 3, 4] Yes, I'm ready to get going! print(*lst) # 1 2 3 4\", 'instruction': 'Suggest an elegant way to print out a list of numbers without using a loop in Python. lst = [1, 2, 3, 4]', 'input': ''}\n",
      "dataset size: 4963\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"flytech/python-codes-25k\", split = \"train\", cache_dir=\"./.cache/huggingface\")\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "#dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "print(len(dataset[\"train\"]))\n",
    "print(len(dataset[\"test\"]))\n",
    "\n",
    "for i in range(5):\n",
    "    print(dataset[\"train\"][i])\n",
    "\n",
    "print(f\"dataset size: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 430/430 [00:00<00:00, 1.46MB/s]\n",
      "Downloading data: 100%|██████████| 33.9M/33.9M [00:02<00:00, 13.9MB/s]\n",
      "Generating train split: 100%|██████████| 49626/49626 [00:00<00:00, 163404.56 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44663\n",
      "4963\n",
      "{'output': \"```python\\nfrom bs4 import BeautifulSoup\\nprint(BeautifulSoup('<p>Hello</p>', 'html.parser'))\\n# Code executed.\\n```\", 'text': \"Execute code: from bs4 import BeautifulSoup\\nprint(BeautifulSoup('<p>Hello</p>', 'html.parser')) Parsing HTML with BeautifulSoup... ```python\\nfrom bs4 import BeautifulSoup\\nprint(BeautifulSoup('<p>Hello</p>', 'html.parser'))\\n# Code executed.\\n```\", 'input': '', 'instruction': \"Execute code: from bs4 import BeautifulSoup\\nprint(BeautifulSoup('<p>Hello</p>', 'html.parser'))\", 'formatted_text': \"<s> [INST] Execute code: from bs4 import BeautifulSoup\\nprint(BeautifulSoup('<p>Hello</p>', 'html.parser')) [/INST] ```python\\nfrom bs4 import BeautifulSoup\\nprint(BeautifulSoup('<p>Hello</p>', 'html.parser'))\\n# Code executed.\\n``` </s>\"}\n",
      "{'output': '```python\\nimport eli5\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.datasets import make_classification\\n\\n# Creating a dataset\\nX, y = make_classification()\\n\\n# Fitting the model\\nmodel = LogisticRegression()\\nmodel.fit(X, y)\\n\\n# Showing model weights\\neli5.show_weights(model)\\n\\n# Let me examine if the code works\\n# 1. Imported required modules\\n# 2. Created a dataset and fitted the model\\n# 3. Showed model weights\\n# Executing code...\\n# Code has been fixed!\\n```', 'text': 'Fix the code: import eli5\\nfrom sklearn.linear_model import LogisticRegression\\nmodel = LogisticRegression()\\neli5.show_weights(model) Fixing the code by fitting the model before calling show_weights... ```python\\nimport eli5\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.datasets import make_classification\\n\\n# Creating a dataset\\nX, y = make_classification()\\n\\n# Fitting the model\\nmodel = LogisticRegression()\\nmodel.fit(X, y)\\n\\n# Showing model weights\\neli5.show_weights(model)\\n\\n# Let me examine if the code works\\n# 1. Imported required modules\\n# 2. Created a dataset and fitted the model\\n# 3. Showed model weights\\n# Executing code...\\n# Code has been fixed!\\n```', 'input': '', 'instruction': 'Fix the code: import eli5\\nfrom sklearn.linear_model import LogisticRegression\\nmodel = LogisticRegression()\\neli5.show_weights(model)', 'formatted_text': '<s> [INST] Fix the code: import eli5\\nfrom sklearn.linear_model import LogisticRegression\\nmodel = LogisticRegression()\\neli5.show_weights(model) [/INST] ```python\\nimport eli5\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.datasets import make_classification\\n\\n# Creating a dataset\\nX, y = make_classification()\\n\\n# Fitting the model\\nmodel = LogisticRegression()\\nmodel.fit(X, y)\\n\\n# Showing model weights\\neli5.show_weights(model)\\n\\n# Let me examine if the code works\\n# 1. Imported required modules\\n# 2. Created a dataset and fitted the model\\n# 3. Showed model weights\\n# Executing code...\\n# Code has been fixed!\\n``` </s>'}\n",
      "{'output': '```python\\ndef shell_sort(arr):\\n    n = len(arr)\\n    gap = n//2\\n \\n    while gap > 0:\\n \\n        for i in range(gap,n):\\n            temp = arr[i]\\n            j = i\\n            while  j >= gap and arr[j-gap] >temp:\\n                arr[j] = arr[j-gap]\\n                j -= gap\\n            arr[j] = temp\\n        gap //= 2\\n```', 'text': 'Rewrite the code to implement a shell sort algorithm in Python. Yes, I can handle this! ```python\\ndef shell_sort(arr):\\n    n = len(arr)\\n    gap = n//2\\n \\n    while gap > 0:\\n \\n        for i in range(gap,n):\\n            temp = arr[i]\\n            j = i\\n            while  j >= gap and arr[j-gap] >temp:\\n                arr[j] = arr[j-gap]\\n                j -= gap\\n            arr[j] = temp\\n        gap //= 2\\n```', 'input': '', 'instruction': 'Rewrite the code to implement a shell sort algorithm in Python.', 'formatted_text': '<s> [INST] Rewrite the code to implement a shell sort algorithm in Python. [/INST] ```python\\ndef shell_sort(arr):\\n    n = len(arr)\\n    gap = n//2\\n \\n    while gap > 0:\\n \\n        for i in range(gap,n):\\n            temp = arr[i]\\n            j = i\\n            while  j >= gap and arr[j-gap] >temp:\\n                arr[j] = arr[j-gap]\\n                j -= gap\\n            arr[j] = temp\\n        gap //= 2\\n``` </s>'}\n",
      "{'output': \"```python\\nimport sys\\nimport logging\\n\\ndef exit_program():\\n    logging.basicConfig(filename='app.log', level=logging.INFO)\\n    exit_message = 'Exiting the program.'\\n    logging.info(exit_message)\\n    sys.exit(exit_message)\\n\\n# 1. Added logging to keep track of exit messages.\\n# 2. Configured logging to write to 'app.log'.\\n# 3. Logged the exit message before calling sys.exit.\\n# Executing code.\\n```\", 'text': \"Fix the code: import sys\\nsys.exit('Exiting the program.') Modify the code to log the exit message before exiting... ```python\\nimport sys\\nimport logging\\n\\ndef exit_program():\\n    logging.basicConfig(filename='app.log', level=logging.INFO)\\n    exit_message = 'Exiting the program.'\\n    logging.info(exit_message)\\n    sys.exit(exit_message)\\n\\n# 1. Added logging to keep track of exit messages.\\n# 2. Configured logging to write to 'app.log'.\\n# 3. Logged the exit message before calling sys.exit.\\n# Executing code.\\n```\", 'input': '', 'instruction': \"Fix the code: import sys\\nsys.exit('Exiting the program.')\", 'formatted_text': \"<s> [INST] Fix the code: import sys\\nsys.exit('Exiting the program.') [/INST] ```python\\nimport sys\\nimport logging\\n\\ndef exit_program():\\n    logging.basicConfig(filename='app.log', level=logging.INFO)\\n    exit_message = 'Exiting the program.'\\n    logging.info(exit_message)\\n    sys.exit(exit_message)\\n\\n# 1. Added logging to keep track of exit messages.\\n# 2. Configured logging to write to 'app.log'.\\n# 3. Logged the exit message before calling sys.exit.\\n# Executing code.\\n``` </s>\"}\n",
      "{'output': '```python\\n# Pattern \\nfor i in range(1,6):\\n for j in range(1,i+1):\\n  print(i,end=\"\")\\n print(\"\")\\n```', 'text': 'Create a Python code for printing the following pattern. Certainly, I can do that for you! ```python\\n# Pattern \\nfor i in range(1,6):\\n for j in range(1,i+1):\\n  print(i,end=\"\")\\n print(\"\")\\n```', 'input': '', 'instruction': 'Create a Python code for printing the following pattern.', 'formatted_text': '<s> [INST] Create a Python code for printing the following pattern. [/INST] ```python\\n# Pattern \\nfor i in range(1,6):\\n for j in range(1,i+1):\\n  print(i,end=\"\")\\n print(\"\")\\n``` </s>'}\n"
     ]
    }
   ],
   "source": [
    "# from datasets import load_dataset\n",
    "# from random import randint\n",
    "\n",
    "# from datasets import load_dataset\n",
    "# eval_dataset = load_dataset(\"sg247/python-codes-25k-llama2\", split = \"train\")\n",
    "# eval_dataset = eval_dataset.train_test_split(test_size=0.1, shuffle=True)\n",
    "# #dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "# print(len(eval_dataset[\"train\"]))\n",
    "# print(len(eval_dataset[\"test\"]))\n",
    "# rand_idx = randint(0, len(eval_dataset))\n",
    "\n",
    "# for i in range(5):\n",
    "#     print(eval_dataset[\"train\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qvr905/.conda/envs/mis/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/qvr905/.conda/envs/mis/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:509: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `100` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction:Generate a Python script for setting up a basic web server for hosting static content\n",
      "Ground truth output:\n",
      "```python\n",
      "#!/usr/bin/env python\n",
      "\n",
      "\n",
      "import http.server\n",
      "\n",
      "# Set the port number\n",
      "port_number = 8080\n",
      "\n",
      "# Create the HTTP server\n",
      "server_address = ('', port_number)\n",
      "httpd = http.server.HTTPServer(server_address, http.server.SimpleHTTPRequestHandler)\n",
      "\n",
      "print('Server started on port {}'.format(str(port_number)))\n",
      "\n",
      "# Run the HTTP server\n",
      "httpd.serve_forever()\n",
      "```\n",
      "Generated Answer:\n",
      "Generate a Python script for setting up a basic web server for hosting static content Generate a Python script for setting up a basic web server for hosting static content... ```python\n",
      "import http.server\n",
      "import socketserver\n",
      "PORT = 8000\n",
      "Handler = http.server.SimpleHTTPRequestHandler\n",
      "with socketserver.TCPServer(('', PORT), Handler) as httpd:\n",
      "    print(f'Serving at port {PORT}')\n",
      "    httpd.serve_forever()\n",
      "```\n",
      "```python\n",
      "import http.server\n",
      "import socketserver\n",
      "PORT = 8000\n",
      "Handler = http.server.SimpleHTTPRequestHandler\n",
      "with socketserver.TCPServer(('', PORT), Handler) as httpd:\n",
      "    print(f'Serving at port {PORT}')\n",
      "    httpd.serve_forever()\n",
      "```\n",
      "# Output: Serving at port 8000\n",
      "```python\n",
      "import http.server\n",
      "import socketserver\n",
      "PORT = 8000\n",
      "Handler = http.server.SimpleHTTPRequestHandler\n",
      "with socketserver.TCPServer(('', PORT), Handler) as httpd:\n",
      "    print(f'Serving at port {PORT}')\n",
      "    httpd.serve_\n",
      "sample:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction:Fix the code: import torch\n",
      "import torch.nn.functional as F\n",
      "input_data = torch.tensor([1.0, 0.5])\n",
      "weights = torch.tensor([0.1, 0.2])\n",
      "output = F.linear(input_data, weights)\n",
      "Ground truth output:\n",
      "```python\n",
      "import torch\n",
      "import torch.nn.functional as F\n",
      "\n",
      "# Creating input data and weights tensors\n",
      "input_data = torch.tensor([1.0, 0.5])\n",
      "weights = torch.tensor([0.1, 0.2])\n",
      "\n",
      "# Performing linear transformation\n",
      "output = F.linear(input_data, weights)\n",
      "\n",
      "# Let me examine if the code works\n",
      "# 1. Performed a linear transformation using functional API\n",
      "# Executing code...\n",
      "# Code has been fixed!\n",
      "```\n",
      "Generated Answer:\n",
      "Fix the code: import torch\n",
      "import torch.nn.functional as F\n",
      "input_data = torch.tensor([1.0, 0.5])\n",
      "weights = torch.tensor([0.1, 0.2])\n",
      "output = F.linear(input_data, weights) Fixing the code by performing a linear transformation using PyTorch... ```python\n",
      "import torch\n",
      "import torch.nn.functional as F\n",
      "\n",
      "input_data = torch.tensor([1.0, 0.5])\n",
      "weights = torch.tensor([0.1, 0.2])\n",
      "output = F.linear(input_data, weights)\n",
      "\n",
      "# Let me examine if the code works\n",
      "# 1. Performed a linear transformation\n",
      "# Executing code...\n",
      "# Code has been fixed!\n",
      "```\n",
      "```python\n",
      "import torch\n",
      "import torch.nn.functional as F\n",
      "\n",
      "input_data = torch.tensor([1.0, 0.5])\n",
      "weights = torch.tensor([0.1, 0.2])\n",
      "output = F.linear(input_data, weights)\n",
      "\n",
      "# Let me examine if the code works\n",
      "# 1. Performed a linear transformation\n",
      "# Executing code...\n",
      "# Code has been fixed!\n",
      "```\n",
      "```python\n",
      "import torch\n",
      "import torch.nn.functional as F\n",
      "\n",
      "input_data = torch.tensor([1.0, 0.5])\n",
      "weights = torch.\n",
      "sample:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction:List out all the sorting algorithms supported in Python.\n",
      "Ground truth output:\n",
      "The sorting algorithms supported in Python include bubble sort, insertion sort, selection sort, merge sort, quick sort, and heap sort.\n",
      "Generated Answer:\n",
      "List out all the sorting algorithms supported in Python. Listing out all the sorting algorithms supported in Python... ```python\n",
      "sorting_algorithms = ['bubble_sort', 'insertion_sort', 'selection_sort', 'merge_sort', 'quick_sort']\n",
      "```\n",
      "### Output: ['bubble_sort', 'insertion_sort', 'selection_sort', 'merge_sort', 'quick_sort']\n",
      "```python\n",
      "sorting_algorithms = ['bubble_sort', 'insertion_sort', 'selection_sort', 'merge_sort', 'quick_sort']\n",
      "```\n",
      "### Output: ['bubble_sort', 'insertion_sort', 'selection_sort', 'merge_sort', 'quick_sort']\n",
      "```python\n",
      "sorting_algorithms = ['bubble_sort', 'insertion_sort', 'selection_sort', 'merge_sort', 'quick_sort']\n",
      "```\n",
      "### Output: ['bubble_sort', 'insertion_sort', 'selection_sort', 'merge_sort', 'quick_sort']\n",
      "```python\n",
      "sorting_algorithms = ['bubble_sort', 'insertion_sort', 'selection_sort', '\n",
      "sample:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction:Write a Python program to compute the sum of the absolute difference of all pairs of numbers in a given array [5, 8, 10, 15]\n",
      "Ground truth output:\n",
      "```python\n",
      "arr = [5, 8, 10, 15]\n",
      "Sum = 0\n",
      "\n",
      "for i in range(len(arr)):\n",
      "    for j in range(len(arr)):\n",
      "        if(i != j):\n",
      "            Sum += abs(arr[i] - arr[j])\n",
      "            \n",
      "print(Sum)\n",
      "```\n",
      "Generated Answer:\n",
      "Write a Python program to compute the sum of the absolute difference of all pairs of numbers in a given array [5, 8, 10, 15] Writeing a Python program to compute the sum of the absolute difference of all pairs of numbers in a given array... ```python\n",
      "arr = [5, 8, 10, 15]\n",
      "sum = 0\n",
      "for i in range(len(arr)):\n",
      "    for j in range(i + 1, len(arr)):\n",
      "        sum += abs(arr[i] - arr[j])\n",
      "print(sum)\n",
      "```\n",
      "# Output: 30\n",
      "```python\n",
      "arr = [5, 8, 10, 15]\n",
      "sum = 0\n",
      "for i in range(len(arr)):\n",
      "    for j in range(i + 1, len(arr)):\n",
      "        sum += abs(arr[i] - arr[j])\n",
      "print(sum)\n",
      "```\n",
      "# Output: 30\n",
      "```python\n",
      "arr = [5, 8, 10, 15]\n",
      "sum = 0\n",
      "for i in range(len(arr)):\n",
      "    for j in range(i + 1, len(arr)):\n",
      "        sum += abs(arr[i] - arr[j])\n",
      "print(\n",
      "sample:3\n",
      "Instruction:Given a set X, suggest a Python program to calculate the median of X\n",
      "Ground truth output:\n",
      "```python\n",
      "def median(X): \n",
      "    sorted_X = sorted(X) \n",
      "    mid_index = (len(sorted_X) - 1) // 2 \n",
      "    if len(sorted_X) % 2 == 0: \n",
      "        return (sorted_X[mid_index] + sorted_X[mid_index + 1]) / 2\n",
      "    else: \n",
      "        return sorted_X[mid_index]\n",
      "```\n",
      "Generated Answer:\n",
      "Given a set X, suggest a Python program to calculate the median of X Write the code. Setting the tempo, let's pioneer the future! ```python\n",
      "def median(X):\n",
      "    X.sort()\n",
      "    if len(X) % 2 == 0:\n",
      "        return (X[len(X)//2] + X[len(X)//2 - 1]) / 2\n",
      "    else:\n",
      "        return X[len(X)//2]\n",
      "```\n",
      "```python\n",
      "X = [1, 2, 3, 4, 5]\n",
      "print(median(X)) # Output: 3.0\n",
      "```\n",
      "```python\n",
      "X = [1, 2, 3, 4, 5]\n",
      "print(median(X)) # Output: 3.0\n",
      "```\n",
      "```python\n",
      "X = [1, 2, 3, 4, 5]\n",
      "print(median(X)) # Output: 3.0\n",
      "```\n",
      "```python\n",
      "X = [1, 2, 3, 4, 5]\n",
      "print(median(X)) # Output: 3.0\n",
      "```\n",
      "```\n",
      "sample:4\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# Initialize lists to store results\n",
    "pred = []\n",
    "iter = 0\n",
    "gt =[]\n",
    "# Iterate through the queries\n",
    "number_of_eval_samples = 50\n",
    "\n",
    "dataseteval = eval_dataset.select(range(number_of_eval_samples))\n",
    "for query in dataseteval:  \n",
    "    instruction = query[\"instruction\"]\n",
    "    ground = query[\"output\"]\n",
    "    gt.append(query[\"output\"])\n",
    "        \n",
    "    input_prompt = f\"{instruction}\"\n",
    "    respons = generate_response(model, tokenizer, input_prompt)\n",
    "    #print(respons)\n",
    "    generated_text = respons\n",
    "    pred.append(generated_text)\n",
    "\n",
    "    # Print the generated text\n",
    "    print(f\"Instruction:{instruction}\")\n",
    "    print(f\"Ground truth output:\\n{ground}\")    \n",
    "    print(f\"Generated Answer:\\n{generated_text}\")\n",
    "    print(f\"sample:{iter}\")\n",
    "    iter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/qvr905/.conda/envs/mis/lib/python3.10/site-packages/tree_sitter/__init__.py:36: FutureWarning: Language(path, name) is deprecated. Use Language(ptr, name) instead.\n",
      "  warn(\"{} is deprecated. Use {} instead.\".format(old, new), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.013837160660285603\n",
      "0.1090903436809324\n",
      "{'codebleu': 0.13331116210210073, 'ngram_match_score': 0.013591339338793447, 'weighted_ngram_match_score': 0.08812219891198744, 'syntax_match_score': 0.10194730813287514, 'dataflow_match_score': 0.3295838020247469}\n",
      "0.6861045956611633 0.7815998792648315 0.7299688458442688\n"
     ]
    }
   ],
   "source": [
    "# Evaluate metrics\n",
    "bleu, rouge ,codebleu, bert_score_precision, bert_score_recall, bert_score_f1= evaluate(pred, gt,model)\n",
    "print(str(bleu))\n",
    "print(str(rouge))\n",
    "print(str(codebleu))\n",
    "print(bert_score_precision.mean().item(), bert_score_recall.mean().item(), bert_score_f1.mean().item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize lists to store results\n",
    "# import gc\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# iter = 0\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# gt =[]\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# # Iterate through the queries\n",
    "# number_of_eval_samples = 100\n",
    "# dataseteval = eval_dataset.shuffle().select(range(number_of_eval_samples))\n",
    "# instr=[]\n",
    "# out = []\n",
    "# for sample in dataseteval:\n",
    "#     instruction = sample[\"instruction\"]\n",
    "#       # Define your prompt using the instruction variable\n",
    "#     prompt = f\"\"\"<start_of_turn>{instruction}<end_of_turn>\n",
    "#     <start_of_turn>model\"\"\"\n",
    "#     input_prompt = f\"<s>[INST] {prompt} [/INST]\"\n",
    "    \n",
    "#     instr.append(input_prompt)\n",
    "#     gt.append(sample['output'])\n",
    "# print(len(instr))\n",
    "\n",
    "# batch_size = 3\n",
    "# for sample in tqdm(list(chunks(instr,batch_size))):\n",
    "    \n",
    "#     encoding = tokenizer(instr, return_tensors=\"pt\",padding = True, truncation = True).to(device)\n",
    "#     with torch.no_grad():\n",
    "#         genereated_ids = model.generate(**encoding,max_new_tokens =256, temperature = 0.9)\n",
    "#     generated_texts = tokenizer.batch_decode(genereated_ids, skip_special_tokens=True)\n",
    "\n",
    "# for sample in dataseteval:\n",
    "#     instruction = sample[\"instruction\"]\n",
    "#     ground = sample[\"output\"]\n",
    "    \n",
    "#     # Print the generated text\n",
    "#     print(f\"Instruction:{instruction}\")\n",
    "#     print(f\"Ground truth output:\\n{ground}\")    \n",
    "#     print(f\"Generated Answer:\\n{generated_texts[iter]}\")\n",
    "#     iter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 5.94k/5.94k [00:00<00:00, 11.1MB/s]\n",
      "Downloading extra modules: 4.07kB [00:00, 4.07MB/s]                   \n",
      "Downloading extra modules: 100%|██████████| 3.34k/3.34k [00:00<00:00, 8.24MB/s]\n",
      "Downloading builder script: 100%|██████████| 6.27k/6.27k [00:00<00:00, 12.2MB/s]\n",
      "Downloading builder script: 100%|██████████| 7.95k/7.95k [00:00<00:00, 6.10MB/s]\n",
      "Downloading builder script: 100%|██████████| 5.57k/5.57k [00:00<00:00, 8.03MB/s]\n"
     ]
    }
   ],
   "source": [
    "# from tqdm import tqdm\n",
    "# import evaluate\n",
    "# from codebleu import calc_codebleu\n",
    "\n",
    "# # Load evaluation metrics\n",
    "# bleu_metric = evaluate.load(\"bleu\")\n",
    "# rouge_metric = evaluate.load(\"rouge\")\n",
    "# bert_metric = evaluate.load(\"bertscore\")\n",
    "# codebleu_metric = evaluate.load(\"k4black/codebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval = evaluate.evaluator(\"text-generation\")\n",
    "# result = eval.compute(\n",
    "#     model_or_pipeline=pipe,\n",
    "#     data=eval_dataset[\"test\"],\n",
    "#     metric=bleu_metric,\n",
    "#     input_column=\"\", \n",
    "#     label_column = \"\"\n",
    "# )\n",
    "\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction:Utilize the Python in-built module to print the calendar for the given month Month: 5\n",
      "Year: 2021\n",
      "Ground truth output:\n",
      "```python\n",
      "# import the module\n",
      "import calendar\n",
      "\n",
      "# print the calendar \n",
      "print(calendar.month(2021, 5))\n",
      "```\n",
      "Generated Answer:\n",
      "import calendar\n",
      "c = calendar.TextCalendar(calendar.SUNDAY)\n",
      "str(c.formatmonth(2021, 5))\n"
     ]
    }
   ],
   "source": [
    "# # Fetch instruction from eval_dataset\n",
    "# instruction = eval_dataset['test'][rand_idx][\"instruction\"]\n",
    "# groundtruth = eval_dataset['test'][rand_idx][\"output\"]\n",
    "\n",
    "# # Define your prompt using the instruction variable\n",
    "# prompt = f\"\"\"<start_of_turn>{instruction}<end_of_turn>\n",
    "# <start_of_turn>model\"\"\"\n",
    "\n",
    "# # Use the pipeline for text generation\n",
    "# pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
    "# result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "# generated_text = result[0]['generated_text']\n",
    "# generated_code = generated_text.split(\"```python\")[1].split(\"```\")[0].strip()\n",
    "# # Print the generated text\n",
    "# #print(result[0]['generated_text'])\n",
    "# print(f\"Instruction:{instruction}\")\n",
    "# print(f\"Ground truth output:\\n{groundtruth}\")\n",
    "# print(f\"Generated Answer:\\n{generated_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:question: {'text': \"Write a Python script which checks if an input string is a palindrome. Yes, let's get down to business! ```python\\ndef is_palindrome(input_string):\\n    input_string = input_string.lower()\\n    if len(input_string) == 0:\\n        return False\\n    if len(input_string) == 1:\\n        return True\\n\\n    if input_string[0] == input_string[-1]:\\n        return is_palindrome(input_string[1:-1])\\n\\n    return False\\n\\n# Usage \\nstring = 'RADAR'\\nresult = is_palindrome(string)\\nprint(result) # Output: True\\n```\", 'input': '', 'output': \"```python\\ndef is_palindrome(input_string):\\n    input_string = input_string.lower()\\n    if len(input_string) == 0:\\n        return False\\n    if len(input_string) == 1:\\n        return True\\n\\n    if input_string[0] == input_string[-1]:\\n        return is_palindrome(input_string[1:-1])\\n\\n    return False\\n\\n# Usage \\nstring = 'RADAR'\\nresult = is_palindrome(string)\\nprint(result) # Output: True\\n```\", 'instruction': 'Write a Python script which checks if an input string is a palindrome.'}\n",
      "Response Llama: question: {'text': \"Write a Python script which checks if an input string is a palindrome. Yes, let's get down to business! ```python\\ndef is_palindrome(input_string):\\n    input_string = input_string.lower()\\n    if len(input_string) == 0:\\n        return False\\n    if len(input_string) == 1:\\n        return True\\n\\n    if input_string[0] == input_string[-1]:\\n        return is_palindrome(input_string[1:-1])\\n\\n    return False\\n\\n# Usage \\nstring = 'RADAR'\\nresult = is_palindrome(string)\\nprint(result) # Output: True\\n```\", 'input': '', 'output': \"```python\\ndef is_palindrome(input_string):\\n    input_string = input_string.lower()\\n    if len(input_string) == 0:\\n        return False\\n    if len(input_string) == 1:\\n        return True\\n\\n    if input_string[0] == input_string[-1]:\\n        return is_palindrome(input_string[1:-1])\\n\\n    return False\\n\\n# Usage \\nstring = 'RADAR'\\nresult = is_palindrome(string)\\nprint(result) # Output: True\\n```\", 'instruction': 'Write a Python script which checks if an input string is a palindrome.'} Write a Python script which checks if an input string is a palindrome. Yes, let's get down to business! ```python\n",
      "def is_palindrome(input_string):\n",
      "    input_string = input_string.lower()\n",
      "    if len(input_string) == 0:\n",
      "        return False\n",
      "    if len(input_string) == 1:\n",
      "        return True\n",
      "    if input_string[0] == input_string[-1]:\n",
      "        return is_palindrome(input_string[1:-1])\n",
      "    return False\n",
      "\n",
      "# Usage\n",
      "string = 'RADAR'\n",
      "result = is_palindrome(string)\n",
      "print(result) # Output: True\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "``\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:question: {'text': \"Write a python script to compare two directories and output the differences dir1/\\n        file1.txt\\n        file2.txt\\n\\ndir2/\\n        file1.txt\\n        file3.txt Consider it in progress! Working on it now! ```python\\nimport os\\n\\ndef compare_dirs(dir1, dir2):\\n    dir1_listing = os.listdir(dir1)\\n    dir2_listing = os.listdir(dir2)\\n\\n    dir1_files = set(dir1_listing)\\n    dir2_files = set(dir2_listing)\\n\\n    new_files = dir2_files.difference(dir1_files)\\n    deleted_files = dir1_files.difference(dir2_files)\\n\\n    print('NEW FILES:')\\n    for f in new_files:\\n        print(f)\\n\\n    print('DELETED FILES:')\\n    for f in deleted_files:\\n        print(f)\\n\\n\\ncompare_dirs('dir1/', 'dir2/')\\n```\", 'input': '', 'output': \"```python\\nimport os\\n\\ndef compare_dirs(dir1, dir2):\\n    dir1_listing = os.listdir(dir1)\\n    dir2_listing = os.listdir(dir2)\\n\\n    dir1_files = set(dir1_listing)\\n    dir2_files = set(dir2_listing)\\n\\n    new_files = dir2_files.difference(dir1_files)\\n    deleted_files = dir1_files.difference(dir2_files)\\n\\n    print('NEW FILES:')\\n    for f in new_files:\\n        print(f)\\n\\n    print('DELETED FILES:')\\n    for f in deleted_files:\\n        print(f)\\n\\n\\ncompare_dirs('dir1/', 'dir2/')\\n```\", 'instruction': 'Write a python script to compare two directories and output the differences dir1/\\n        file1.txt\\n        file2.txt\\n\\ndir2/\\n        file1.txt\\n        file3.txt'}\n",
      "Response Llama: question: {'text': \"Write a python script to compare two directories and output the differences dir1/\\n        file1.txt\\n        file2.txt\\n\\ndir2/\\n        file1.txt\\n        file3.txt Consider it in progress! Working on it now! ```python\\nimport os\\n\\ndef compare_dirs(dir1, dir2):\\n    dir1_listing = os.listdir(dir1)\\n    dir2_listing = os.listdir(dir2)\\n\\n    dir1_files = set(dir1_listing)\\n    dir2_files = set(dir2_listing)\\n\\n    new_files = dir2_files.difference(dir1_files)\\n    deleted_files = dir1_files.difference(dir2_files)\\n\\n    print('NEW FILES:')\\n    for f in new_files:\\n        print(f)\\n\\n    print('DELETED FILES:')\\n    for f in deleted_files:\\n        print(f)\\n\\n\\ncompare_dirs('dir1/', 'dir2/')\\n```\", 'input': '', 'output': \"```python\\nimport os\\n\\ndef compare_dirs(dir1, dir2):\\n    dir1_listing = os.listdir(dir1)\\n    dir2_listing = os.listdir(dir2)\\n\\n    dir1_files = set(dir1_listing)\\n    dir2_files = set(dir2_listing)\\n\\n    new_files = dir2_files.difference(dir1_files)\\n    deleted_files = dir1_files.difference(dir2_files)\\n\\n    print('NEW FILES:')\\n    for f in new_files:\\n        print(f)\\n\\n    print('DELETED FILES:')\\n    for f in deleted_files:\\n        print(f)\\n\\n\\ncompare_dirs('dir1/', 'dir2/')\\n```\", 'instruction': 'Write a python script to compare two directories and output the differences dir1/\\n        file1.txt\\n        file2.txt\\n\\ndir2/\\n        file1.txt\\n        file3.txt'} Write a python script to compare two directories and output the differences dir1/\\n        file1.txt\\n        file2.txt\\n\\ndir2/\\n        file1.txt\\n        file3.txt. ```python\\nimport os\\n\\ndef compare_dirs(dir1, dir2):\\n    dir1_listing = os.listdir(dir1)\\n    dir2_listing = os.listdir(dir2)\\n\\n    dir1_files = set(dir1_listing)\\n    dir2_files = set(dir2_listing)\\n\\n    new_files = dir2_files.difference(dir1_files)\\n    deleted_files = dir1_files.difference(dir2_files)\\n\\n    print('NEW FILES:')\\n    for f in new_files:\\n        print(f)\\n\\n    print('DELETED FILES:')\\n    for f in deleted_files:\\n        print(f)\\n\\n\\ncompare_dirs('dir1/', 'dir2/')\\n```\", 'output': \"```python\\nimport os\\n\\ndef compare_dirs(dir1, dir2):\\n    dir1_listing = os.listdir(dir1)\\n    dir2_listing = os.listdir(dir2)\\n\\n    dir1_files = set(dir1_listing)\\n    dir2_files = set(dir2_listing)\\n\\n    new_files = dir2_files.difference(dir1_files)\\n    deleted_files = dir1_files.difference(dir2_files)\\n\\n    print('NEW FILES:')\\n    for f in new_files:\\n        print(f)\\n\\n    print('DELETED FILES:')\\n    for f in deleted_files:\\n        print(f)\\n\\n\\ncompare_dirs('dir1/', 'dir2/')\\n```\", 'instruction': 'Write a python script to compare two direct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:question: {'text': 'Can you write an Python program to find the sum of two given numbers? Firing up the engines! We\\'re almost airborne! ```python\\ndef sum_integers(a, b): \\n    return a + b \\n    \\n# Driver Program \\na = 4\\nb = 5\\n\\nprint(\"Sum of {} and {} is: {}\".format(a, b, sum_integers(a, b)))\\n```', 'input': '', 'output': '```python\\ndef sum_integers(a, b): \\n    return a + b \\n    \\n# Driver Program \\na = 4\\nb = 5\\n\\nprint(\"Sum of {} and {} is: {}\".format(a, b, sum_integers(a, b)))\\n```', 'instruction': 'Can you write an Python program to find the sum of two given numbers?'}\n",
      "Response Llama: question: {'text': 'Can you write an Python program to find the sum of two given numbers? Firing up the engines! We\\'re almost airborne! ```python\\ndef sum_integers(a, b): \\n    return a + b \\n    \\n# Driver Program \\na = 4\\nb = 5\\n\\nprint(\"Sum of {} and {} is: {}\".format(a, b, sum_integers(a, b)))\\n```', 'input': '', 'output': '```python\\ndef sum_integers(a, b): \\n    return a + b \\n    \\n# Driver Program \\na = 4\\nb = 5\\n\\nprint(\"Sum of {} and {} is: {}\".format(a, b, sum_integers(a, b)))\\n```', 'instruction': 'Can you write an Python program to find the sum of two given numbers?'} Write a Python program to find the sum of two given numbers.\n",
      "# Driver Program\n",
      "a = 4\n",
      "b = 5\n",
      "\n",
      "print(\"Sum of {} and {} is: {}\".format(a, b, sum_integers(a, b))')\n",
      "```python\n",
      "def sum_integers(a, b):\n",
      "    return a + b\n",
      "\n",
      "# Driver Program\n",
      "a = 4\n",
      "b = 5\n",
      "\n",
      "print(\"Sum of {} and {} is: {}\".format(a, b, sum_integers(a, b)))\n",
      "```\n",
      "# Output:\n",
      "Sum of 4 and 5 is: 9\n",
      "```\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:question: {'text': \"Run Disk Cleanup Running Disk Cleanup... ```python\\nimport os\\nos.system('cleanmgr')\\n```\", 'input': 'Running Disk Cleanup...', 'output': \"```python\\nimport os\\nos.system('cleanmgr')\\n```\", 'instruction': 'Run Disk Cleanup'}\n",
      "Response Llama: question: {'text': \"Run Disk Cleanup Running Disk Cleanup... ```python\\nimport os\\nos.system('cleanmgr')\\n```\", 'input': 'Running Disk Cleanup...', 'output': \"```python\\nimport os\\nos.system('cleanmgr')\\n```\", 'instruction': 'Run Disk Cleanup'} Write a Python code to run Disk Cleanup. ```python\n",
      "import os\n",
      "os.system('cleanmgr')\n",
      "```\n",
      "Running Disk Cleanup... ```python\n",
      "import os\n",
      "os.system('cleanmgr')\n",
      "```\n",
      "# Output: Running Disk Cleanup...\n",
      "```python\n",
      "import os\n",
      "os.system('cleanmgr')\n",
      "```\n",
      "```python\n",
      "import os\n",
      "os.system('cleanmgr')\n",
      "```\n",
      "# Running Disk Cleanup...\n",
      "```python\n",
      "import os\n",
      "os.system('cleanmgr')\n",
      "```\n",
      "```python\n",
      "import os\n",
      "os.system('cleanmgr')\n",
      "```\n",
      "# Running Disk Cleanup...\n",
      "```python\n",
      "import os\n",
      "os.system('cleanmgr')\n",
      "```\n",
      "# Output: Running Disk Cleanup...\n",
      "```python\n",
      "import os\n",
      "os.system('cleanmgr')\n",
      "```\n",
      "# Running Disk Cleanup...\n",
      "```python\n",
      "import os\n",
      "os.system('cleanmgr')\n",
      "```\n",
      "# Output: Running Disk Cleanup...\n",
      "```python\n",
      "import os\n",
      "os.system('cleanmgr')\n",
      "```\n",
      "# Running Disk Cleanup...\n",
      "```python\n",
      "import os\n",
      "os.system('cleanmgr')\n",
      "```\n",
      "# Output: Running Disk Cleanup...\n",
      "```python\n",
      "import os\n",
      "os.system('cleanmgr')\n",
      "```\n",
      "# Running Disk Cleanup...\n",
      "```python\n",
      "import os\n",
      "os.system('cleanmgr')\n",
      "```\n",
      "# Output: Running Disk Cleanup...\n",
      "```python\n",
      "import os\n",
      "os.system('cleanmgr')\n",
      "```\n",
      "# Running Disk Cleanup...\n",
      "```python\n",
      "import os\n",
      "os.system('cleanmgr')\n",
      "```\n",
      "# Output: Running Disk Cleanup...\n",
      "```python\n",
      "import os\n",
      "os.system('cleanmgr')\n",
      "```\n",
      "# Running Disk Cleanup...\n",
      "```python\n",
      "import os\n",
      "os.system('cleanmgr')\n",
      "```\n",
      "# Output: Running Disk Cleanup...\n",
      "```python\n",
      "import os\n",
      "os.system('cleanmgr')\n",
      "```\n",
      "# Running Disk Cleanup...\n",
      "```python\n",
      "import os\n",
      "os.system('cleanmgr')\n",
      "```\n",
      "# Output: Running Disk Cleanup...\n",
      "```python\n",
      "import os\n",
      "os.system('cleanmgr')\n",
      "```\n",
      "# Running Disk Cleanup...\n",
      "```python\n",
      "import os\n",
      "os.system('cleanmgr')\n",
      "```\n",
      "# Output: Running Disk Cleanup...\n",
      "```python\n",
      "import os\n",
      "os.system('cleanmgr')\n",
      "```\n",
      "# Running Disk Cleanup...\n",
      "```python\n",
      "import os\n",
      "os.system('cleanmgr')\n",
      "```\n",
      "# Output: Running Disk Cleanup...\n",
      "```python\n",
      "import os\n",
      "os.system('cleanmgr')\n",
      "```\n",
      "# Running Disk Cleanup...\n",
      "```python\n",
      "import os\n",
      "os.system('cleanmgr')\n",
      "```\n",
      "# Output: Running Disk Cleanup...\n",
      "```python\n",
      "import os\n",
      "os.system('cleanmgr')\n",
      "```\n",
      "# Running Disk Cleanup...\n",
      "```python\n",
      "import os\n",
      "os.system('cleanmgr')\n",
      "```\n",
      "# Output: Running Disk Cleanup...\n",
      "```python\n",
      "import os\n",
      "os.system('cleanmgr')\n",
      "```\n",
      "# Running Disk Cleanup...\n",
      "```python\n",
      "import os\n",
      "os.system('cleanmgr')\n",
      "```\n",
      "# Output: Running Disk Cleanup...\n",
      "```python\n",
      "import os\n",
      "os.system('cleanmgr')\n",
      "```\n",
      "# Running Disk Cleanup...\n",
      "```python\n",
      "import os\n",
      "os.system('cleanmgr')\n",
      "```\n",
      "# Output: Running Disk Cleanup...\n",
      "```python\n",
      "import os\n",
      "os.system('cleanmgr')\n",
      "```\n",
      "# Running Disk Cleanup...\n",
      "```python\n",
      "import os\n",
      "os.system('cleanmgr')\n",
      "```\n",
      "# Output: Running Disk Cleanup...\n",
      "```python\n",
      "import os\n",
      "os.system('cleanmgr')\n",
      "```\n",
      "# Running Disk Cleanup...\n",
      "```python\n",
      "import os\n",
      "os.system('cleanmgr')\n",
      "```\n",
      "# Output: Running Disk Cleanup...\n",
      "```python\n",
      "import os\n",
      "Question:question: {'text': \"Execute code: from bs4 import BeautifulSoup\\nsoup = BeautifulSoup('<html><head></head><body></body></html>', 'html.parser')\\nprint(soup.prettify()) Creating and pretty-printing a BeautifulSoup object... ```python\\nfrom bs4 import BeautifulSoup\\nsoup = BeautifulSoup('<html><head></head><body></body></html>', 'html.parser')\\nprint(soup.prettify())\\n# Code executed. BeautifulSoup object pretty-printed.\\n```\", 'input': '', 'output': \"```python\\nfrom bs4 import BeautifulSoup\\nsoup = BeautifulSoup('<html><head></head><body></body></html>', 'html.parser')\\nprint(soup.prettify())\\n# Code executed. BeautifulSoup object pretty-printed.\\n```\", 'instruction': \"Execute code: from bs4 import BeautifulSoup\\nsoup = BeautifulSoup('<html><head></head><body></body></html>', 'html.parser')\\nprint(soup.prettify())\"}\n",
      "Response Llama: question: {'text': \"Execute code: from bs4 import BeautifulSoup\\nsoup = BeautifulSoup('<html><head></head><body></body></html>', 'html.parser')\\nprint(soup.prettify()) Creating and pretty-printing a BeautifulSoup object... ```python\\nfrom bs4 import BeautifulSoup\\nsoup = BeautifulSoup('<html><head></head><body></body></html>', 'html.parser')\\nprint(soup.prettify())\\n# Code executed. BeautifulSoup object pretty-printed.\\n```\", 'input': '', 'output': \"```python\\nfrom bs4 import BeautifulSoup\\nsoup = BeautifulSoup('<html><head></head><body></body></html>', 'html.parser')\\nprint(soup.prettify())\\n# Code executed. BeautifulSoup object pretty-printed.\\n```\", 'instruction': \"Execute code: from bs4 import BeautifulSoup\\nsoup = BeautifulSoup('<html><head></head><body></body></html>', 'html.parser')\\nprint(soup.prettify())\"} Write a Python code to create and pretty-print a BeautifulSoup object... ```python\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup('<html><head></head><body></body></html>', 'html.parser')\n",
      "print(soup.prettify())\n",
      "# Code executed. BeautifulSoup object pretty-printed.\n",
      "````, 'language': 'Python', 'response_type': 'text', 'id': 'ktype:q:p:12345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# # Initialize lists to store results\n",
    "# responses = []\n",
    "# iter = 0\n",
    "# # Iterate through the queries\n",
    "# number_of_eval_samples = 5\n",
    "# dataseteval = eval_dataset['test'].shuffle().select(range(number_of_eval_samples))\n",
    "# for query in dataseteval:  \n",
    "#     instruction = query[\"instruction\"]\n",
    "#     groundtruth = query[\"output\"]\n",
    "    \n",
    "#     # Define your prompt using the instruction variable\n",
    "#     prompt = f\"\"\"<start_of_turn>{instruction}<end_of_turn>\n",
    "#     <start_of_turn>model\"\"\"\n",
    "#     input_prompt = f\"<s>[INST] {prompt} [/INST]\"\n",
    "\n",
    "#     # Generate response without RAG\n",
    "#     input_prompt = f\"question: {query}\"\n",
    "#     respons = generate_response(model, tokenizer, input_prompt)\n",
    "    \n",
    "    \n",
    "#     responses.append(respons)\n",
    "#     print(\"Question:\" + input_prompt)\n",
    "#     print(\"Response Llama: \" + respons)\n",
    "#     iter+=1\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References: \n",
    "https://www.philschmid.de/fine-tune-llms-in-2024-with-trl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
