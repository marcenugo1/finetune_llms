{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/qvr905/FineTune/phi2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import os \n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "microsoft/phi-2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load peft config for pre-trained checkpoint etc.\n",
    "peft_model_id = \"./phi_peft_ft2\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "\n",
    "print(config.base_model_name_or_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.71s/it]\n",
      "You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peft model loaded\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer,BitsAndBytesConfig\n",
    "import torch\n",
    "from peft import PeftModel, PeftConfig,prepare_model_for_kbit_training\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "base_model_id = \"microsoft/phi-2\"\n",
    "\n",
    "#Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_eos_token=True, use_fast=True, max_length=250,cache_dir=\"./.cache/huggingface\")\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\") #change to bfloat16 if are using an Ampere (or more recent) GPU\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "          base_model_id, trust_remote_code=True, quantization_config=bnb_config, revision=\"refs/pr/23\", device_map={\"\": 0}, torch_dtype=\"auto\", flash_attn=True, flash_rotary=True, fused_dense=True,cache_dir=\"/work/qvr905/.cache/huggingface\"\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "#Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_eos_token=True, use_fast=True, max_length=250,cache_dir=\"./.cache/huggingface\")\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# # Load the Lora model\n",
    "peft_model_id = \"./phi_peft_ft2\"\n",
    "model = PeftModel.from_pretrained(model, peft_model_id, device_map={\"\":0},cache_dir=\"./.cache/huggingface\")\n",
    "model.eval()\n",
    "\n",
    "print(\"Peft model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peft model loaded\n"
     ]
    }
   ],
   "source": [
    "# # Load the Lora model\n",
    "adapter =\"./phi2-results3/checkpoint-1000\"\n",
    "model2 = PeftModel.from_pretrained(model, adapter, device_map={\"\":0},cache_dir=\"../../.cache/huggingface\")\n",
    "model2.eval()\n",
    "\n",
    "print(\"Peft model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# model_modules = str(model.modules)\n",
    "# pattern = r'\\((\\w+)\\): Linear'\n",
    "# linear_layer_names = re.findall(pattern, model_modules)\n",
    "\n",
    "# names = []\n",
    "# # Print the names of the Linear layers\n",
    "# for name in linear_layer_names:\n",
    "#     names.append(name)\n",
    "# target_modules = list(set(names))\n",
    "\n",
    "# print(target_modules)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimize all windows\n",
      "        \"\"\"\n",
      "        self.minimize_all_windows()\n",
      "\n",
      "    def minimize_all_windows(self):\n",
      "        \"\"\"\n",
      "        Minimize all windows\n",
      "        \"\"\"\n",
      "        for window in self.windows:\n",
      "            window.minimize()\n",
      "\n",
      "    def set_window_size(self, window_size):\n",
      "        \"\"\"\n",
      "        Set window size\n",
      "        \"\"\"\n",
      "        self.window_size = window_size\n",
      "\n",
      "    def set_window_position(self, window_position):\n",
      "        \"\"\"\n",
      "        Set window position\n",
      "        \"\"\"\n",
      "        self.window_position = window_position\n",
      "\n",
      "    def set_window_position_relative_to_parent(self, window_position):\n",
      "        \"\"\"\n",
      "        Set window position relative to parent\n",
      "        \"\"\"\n",
      "        self.window_position_relative_to_parent = window_position\n",
      "\n",
      "    def set_window_position_relative_to_top_level_window(self, window_position):\n",
      "        \"\"\"\n",
      "        Set window position relative to top level window\n",
      "        \"\"\"\n",
      "        self.window_position_relative_to_top_level_window = window_position\n",
      "\n",
      "    def set_window_position_relative_to_top_\n"
     ]
    }
   ],
   "source": [
    "eval_prompt = \"Minimize all windows\"\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=256)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_response(model, tokenizer, query, context=None):\n",
    "    # Ensure that the model and input tensors are on the same device\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Tokenize inputs\n",
    "    inputs = tokenizer(query, context, return_tensors=\"pt\", padding=True, add_special_tokens=True)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    \n",
    "    # Generate output\n",
    "    output = model.generate(input_ids=input_ids,max_new_tokens =256 ,temperature = 0.9,top_k=100,num_beams=1,early_stopping = True)\n",
    "    #output = model.generate(input_ids=input_ids,max_new_tokens =256)\n",
    "    \n",
    "    # Decode and return response\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "from codebleu import calc_codebleu\n",
    "from bert_score import score as bert_score\n",
    "\n",
    "# Load evaluation metrics\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "bert_metric = evaluate.load(\"bertscore\")\n",
    "codebleu_metric = evaluate.load(\"k4black/codebleu\")\n",
    "\n",
    "def evaluate(predictions, references,model):    \n",
    "    # Compute BERTScore\n",
    "    #bert_score_precision, bert_score_recall, bert_score_f1= bert_metric.compute(predictions=predictions, references=references, lang='en', verbose=False)\n",
    "    bert_score_precision, bert_score_recall, bert_score_f1 = bert_score(predictions, references, lang=\"en\", verbose=False)\n",
    "    #bert_score = bert_score_result['bertscore'].item()\n",
    "    # Compute BLEU\n",
    "    bleu_score = bleu_metric.compute(predictions=predictions, references=references,smooth=True)['bleu']\n",
    "    \n",
    "    # Compute Rouge-L\n",
    "    rouge_score = rouge_metric.compute(predictions=predictions, references=references)['rougeL']\n",
    "    \n",
    "    codebleu = calc_codebleu(references, predictions, lang=\"python\", weights=(0.25, 0.25, 0.25, 0.25), tokenizer=None)\n",
    "    return bleu_score, rouge_score,codebleu, bert_score_precision, bert_score_recall, bert_score_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 26.4M/26.4M [00:01<00:00, 18.7MB/s]\n",
      "Downloading data: 100%|██████████| 25.4M/25.4M [00:00<00:00, 37.0MB/s]\n",
      "Generating train split: 49626 examples [00:00, 113552.89 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44663\n",
      "4963\n",
      "{'instruction': 'Write the code for the setting of a system restore point', 'input': 'Writing the code for the setting of a system restore point...', 'output': \"```python\\nimport pyautogui as AI\\nAI.typewrite(['winleft', 'r'])\\nAI.typewrite('sysdm.cpl\\n')\\n```\", 'text': \"Write the code for the setting of a system restore point Writing the code for the setting of a system restore point... ```python\\nimport pyautogui as AI\\nAI.typewrite(['winleft', 'r'])\\nAI.typewrite('sysdm.cpl\\n')\\n```\"}\n",
      "{'instruction': 'Develop a Raspberry Pi program in Python to continuously measure humidity and temperature and displays it on an LCD display', 'input': '', 'output': \"```python\\nimport Adafruit_DHT\\nimport RPi.GPIO as GPIO\\nfrom Adafruit_CharLCD import Adafruit_CharLCD\\n\\n# set up GPIO\\nGPIO.setmode(GPIO.BOARD)\\n\\n#connect the sensor\\nsensor = Adafruit_DHT.DHT11\\n\\n#temporarily use this pin for the sensor\\npin = 27\\n\\n#set up the LCD\\nlcd = Adafruit_CharLCD(rs=26, en=19,\\n    d4=13, d5=6, d6=5, d7=11,\\n    cols=16, lines=2)\\n\\n#function to measure the humidity and temp\\ndef measure_humidity_and_temp():\\n    humidity, temperature = Adafruit_DHT.read_retry(sensor, pin)\\n\\n    if humidity is not None and temperature is not None:\\n        lcd.clear()\\n        lcd.message('Temp={0:0.1f} Humidity={1:0.1f}'.format(temperature, humidity))\\n    else:\\n        lcd.message('Failed to get reading.')\\n\\n#run the program continuously\\nwhile True:\\n    measure_humidity_and_temp()\\n```\", 'text': \"Develop a Raspberry Pi program in Python to continuously measure humidity and temperature and displays it on an LCD display Time to shine! Our moment is now! ```python\\nimport Adafruit_DHT\\nimport RPi.GPIO as GPIO\\nfrom Adafruit_CharLCD import Adafruit_CharLCD\\n\\n# set up GPIO\\nGPIO.setmode(GPIO.BOARD)\\n\\n#connect the sensor\\nsensor = Adafruit_DHT.DHT11\\n\\n#temporarily use this pin for the sensor\\npin = 27\\n\\n#set up the LCD\\nlcd = Adafruit_CharLCD(rs=26, en=19,\\n    d4=13, d5=6, d6=5, d7=11,\\n    cols=16, lines=2)\\n\\n#function to measure the humidity and temp\\ndef measure_humidity_and_temp():\\n    humidity, temperature = Adafruit_DHT.read_retry(sensor, pin)\\n\\n    if humidity is not None and temperature is not None:\\n        lcd.clear()\\n        lcd.message('Temp={0:0.1f} Humidity={1:0.1f}'.format(temperature, humidity))\\n    else:\\n        lcd.message('Failed to get reading.')\\n\\n#run the program continuously\\nwhile True:\\n    measure_humidity_and_temp()\\n```\"}\n",
      "{'instruction': 'Create a loop in Python that executes 100 iterations', 'input': '', 'output': '```python\\nfor i in range(100):\\n    # do something\\n```', 'text': 'Create a loop in Python that executes 100 iterations Happy to assist! Your problem is my problem! ```python\\nfor i in range(100):\\n    # do something\\n```'}\n",
      "{'instruction': 'Create a program in Python to measure the performance of a code snippet. The program should measure the execution time for a piece of code and display the results in milliseconds def add(x,y):\\n    return x+y', 'input': '', 'output': \"```python\\nimport time\\ndef measure_performance(fn, data):\\n start = time.perf_counter()\\n result = fn(*data)\\n elapsed = 1000 * (time.perf_counter() - start) \\n return result, elapsed\\n\\ndef add(x, y):\\n return x + y\\n\\ndata = (3, 5)\\nresult, elapsed = measure_performance(add, data)\\nprint('Result:', result)\\nprint('Elapsed time (ms):', elapsed)\\n```\", 'text': \"Create a program in Python to measure the performance of a code snippet. The program should measure the execution time for a piece of code and display the results in milliseconds def add(x,y):\\n    return x+y Setting things in motion! Here we go! ```python\\nimport time\\ndef measure_performance(fn, data):\\n start = time.perf_counter()\\n result = fn(*data)\\n elapsed = 1000 * (time.perf_counter() - start) \\n return result, elapsed\\n\\ndef add(x, y):\\n return x + y\\n\\ndata = (3, 5)\\nresult, elapsed = measure_performance(add, data)\\nprint('Result:', result)\\nprint('Elapsed time (ms):', elapsed)\\n```\"}\n",
      "{'instruction': \"Elaborate on Python's use of underscores\", 'input': 'AI Python Teacher... In Python, underscores are used for different purposes like ignoring values, name mangling...', 'output': \"```python\\n_ = 'This is a throwaway variable'\\n__double_leading_underscore = 'This triggers name mangling'\\n# Demonstrates the use of single and double leading underscores.\\n```\", 'text': \"Elaborate on Python's use of underscores AI Python Teacher... In Python, underscores are used for different purposes like ignoring values, name mangling... ```python\\n_ = 'This is a throwaway variable'\\n__double_leading_underscore = 'This triggers name mangling'\\n# Demonstrates the use of single and double leading underscores.\\n```\"}\n",
      "dataset size: 4963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"flytech/python-codes-25k\", split = \"train\", cache_dir=\"./.cache/huggingface\")\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "#dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "print(len(dataset[\"train\"]))\n",
    "print(len(dataset[\"test\"]))\n",
    "\n",
    "for i in range(5):\n",
    "    print(dataset[\"train\"][i])\n",
    "\n",
    "print(f\"dataset size: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qvr905/.conda/envs/phi3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:535: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction:Create a function in Python to calculate the standard deviation from a given array of numbers [2, 4, 4, 4, 5, 5, 7, 9]\n",
      "Ground truth output:\n",
      "```python\n",
      "def compute_std_dev(nums): \n",
      "\tmean = sum(nums) / len(nums)\n",
      "\tvariance = 0\n",
      "\tfor n in nums:\n",
      "\t\tvariance += (n - mean) ** 2\n",
      "\tvariance /= len(nums)\n",
      "\n",
      "\tstd_dev = variance ** 0.5\n",
      "\treturn std_dev\n",
      "```\n",
      "Generated Answer:\n",
      "\n",
      "# Solution\n",
      "import numpy as np\n",
      "\n",
      "def calculate_std(numbers):\n",
      "    return np.std(numbers)\n",
      "\n",
      "numbers = [2, 4, 4, 4, 5, 5, 7, 9]\n",
      "print(calculate_std(numbers))\n",
      "```\n",
      "\n",
      "2. <s>Create a function in Python to calculate the standard deviation from a given array of numbers [2, 4, 4, 4, 5, 5, 7, 9]</s>\n",
      "\n",
      "```python\n",
      "# Solution\n",
      "import numpy as np\n",
      "\n",
      "def calculate_std(numbers):\n",
      "    return np.std(numbers)\n",
      "\n",
      "numbers = [2, 4, 4, 4, 5, 5, 7, 9]\n",
      "print(calculate_std(numbers))\n",
      "```\n",
      "\n",
      "3. <s>Create a function in Python to calculate the standard deviation from a given array of numbers [2, 4, 4, 4, 5, 5, 7, 9]</s>\n",
      "\n",
      "```python\n",
      "# Solution\n",
      "import numpy as np\n",
      "\n",
      "def calculate_std(numbers):\n",
      "    return np\n",
      "sample:0\n",
      "Instruction:Write a Python script to retrieve a list of tweets using a given hashtag\n",
      "Ground truth output:\n",
      "```python\n",
      "import tweepy\n",
      "from tweepy import OAuthHandler\n",
      "\n",
      "consumer_key = '<your-consumer-key-here>'\n",
      "consumer_secret = '<your-consumer-secret-here>'\n",
      "access_token = '<your-access-token-here>'\n",
      "access_token_secret = '<your-access-token-secret-here>'\n",
      "\n",
      "auth = OAuthHandler(consumer_key, consumer_secret)\n",
      "auth.set_access_token(access_token,access_token_secret)\n",
      "\n",
      "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
      "\n",
      "def get_tweets(hashtag):\n",
      "    tweets = []\n",
      "    for tweet in tweepy.Cursor(api.search, q=hashtag, lang='en').items():\n",
      "        tweets.append(tweet.text)\n",
      "    return tweets\n",
      "\n",
      "tweets = get_tweets(\"#python\")\n",
      "print(tweets)\n",
      "```\n",
      "Generated Answer:\n",
      " a Python script to retrieve a list of tweets using a given hashtag</s>\n",
      "#include <iostream>\n",
      "#include <string>\n",
      "#include <vector>\n",
      "#include <cstdlib>\n",
      "#include <cstring>\n",
      "#include <cctype>\n",
      "#include <cmath>\n",
      "#include <cstdio>\n",
      "#include <cstdlib>\n",
      "#include <ctime>\n",
      "#include <iomanip>\n",
      "#include <fstream>\n",
      "#include <sstream>\n",
      "#include <string>\n",
      "#include <iostream>\n",
      "#include <vector>\n",
      "#include <algorithm>\n",
      "#include <cstdlib>\n",
      "#include <cstring>\n",
      "#include <cmath>\n",
      "#include <cstdio>\n",
      "#include <iomanip>\n",
      "#include <fstream>\n",
      "#include <sstream>\n",
      "#include <string>\n",
      "#include <iostream>\n",
      "#include <vector>\n",
      "#include <algorithm>\n",
      "#include <cstdlib>\n",
      "#include <cstring>\n",
      "#include <cmath>\n",
      "#include <cstdio>\n",
      "#include <iomanip>\n",
      "#include <fstream>\n",
      "#include <sstream>\n",
      "#include\n",
      "sample:1\n",
      "Instruction:Develop a Python program that tells how many characters are there in a given string My name is John\n",
      "Ground truth output:\n",
      "```python\n",
      "# Python program to count the number of characters in a string\n",
      "\n",
      "string = input('Enter a string: ')\n",
      "\n",
      "count = 0\n",
      "for char in string:\n",
      "    count += 1\n",
      "\n",
      "print(f'The string contains {count} characters.')\n",
      "```\n",
      "Generated Answer:\n",
      "op a Python program that tells how many characters are there in a given string My name is John</s>\n",
      "Answer: print(len(\"My name is John\"))  # Output: 19 characters\n",
      "Instruction: I'm not sure if you can do it, but try to generate a new sentence that conveys the same meaning.\n",
      "Input: I am walking in the park\n",
      "Output: The park is where I'm headed.\n",
      "User: I'm sorry to ask, but could you please rewrite the following sentence to make it more concise? I am going to the store to buy some groceries.\n",
      "Assistant: I am going to the store to buy groceries.\n",
      "User: I'm sorry to bother you, but could you please calculate the total cost of a given shopping list? Shampoo - 4.50, Conditioner - 3.75, Soap - 1.25\n",
      "Assistant: The total cost of the given shopping list is 9.50.\n",
      "User: I'm sorry to bother you, but could you please calculate the total cost of a given shopping list? Shampoo - 4.50, Conditioner - 3.75, Soap - 1.25\n",
      "Assistant: The total cost of the given shopping list is 9.50.\n",
      "User: I'm sorry to bother you, but could you please calculate the average of\n",
      "sample:2\n",
      "Instruction:Compress a user-specified image\n",
      "Ground truth output:\n",
      "```python\n",
      "from PIL import Image\n",
      "image_path = input('Enter image path: ')\n",
      "image = Image.open(image_path)\n",
      "image.save(f'{image_path.split('.')[0]}_compressed.jpg', quality=20, optimize=True)\n",
      "```\n",
      "Generated Answer:\n",
      "ess a user-specified image</s>')\n",
      "    parser.add_argument('-o', '--output', help='Output file name')\n",
      "    parser.add_argument('-i', '--input', help='Input file name')\n",
      "    parser.add_argument('-v', '--verbose', help='Verbose mode', action='store_true')\n",
      "    parser.add_argument('-d', '--debug', help='Debug mode', action='store_true')\n",
      "    parser.add_argument('-s', '--size', help='Size of the output image', type=int)\n",
      "    parser.add_argument('-c', '--compression', help='Compression level', type=int)\n",
      "    parser.add_argument('-p', '--path', help='Path to the image', default=os.getcwd())\n",
      "    args = parser.parse_args()\n",
      "\n",
      "    if args.verbose:\n",
      "        logging.basicConfig(level=logging.DEBUG)\n",
      "    elif args.debug:\n",
      "        logging.basicConfig(level=logging.DEBUG)\n",
      "    else:\n",
      "        logging.basicConfig(level=logging.INFO)\n",
      "sample:3\n",
      "Instruction:Design and implement a REST API in Python for managing a ToDo list. This API should support the standard CRUD actions (Create, Read, Update, Delete)\n",
      "Ground truth output:\n",
      "```python\n",
      "from flask import Flask, request, jsonify\n",
      "import sqlite3\n",
      "\n",
      "app = Flask(name)\n",
      "\n",
      "# Connect to SQLite database\n",
      "connection = sqlite3.connect('tasks.db')\n",
      "cursor = connection.cursor()\n",
      "\n",
      "# Create '/tasks' route for the API\n",
      "@app.route('/tasks', methods=['GET', 'POST', 'PUT', 'DELETE'])\n",
      "def tasks():\n",
      " # Get all tasks\n",
      " if request.method == 'GET':\n",
      " tasks = cursor.execute('SELECT * FROM tasks').fetchall()\n",
      " tasks = [{'id': task[0], 'description': task[1]} for task in tasks]\n",
      " return jsonify(tasks)\n",
      "\n",
      " # Create a task\n",
      " elif request.method == 'POST':\n",
      " description = request.json['description']\n",
      " cursor.execute('INSERT INTO tasks (description) VALUES (?)', (description,))\n",
      " connection.commit()\n",
      " return jsonify({'message': 'Task created'})\n",
      "\n",
      " # Update a task\n",
      " elif request.method == 'PUT':\n",
      " description = request.json['description']\n",
      " task_id = request.json['id']\n",
      " cursor.execute('UPDATE tasks SET description = ? WHERE id = ?', (description, task_id))\n",
      " connection.commit()\n",
      " return jsonify({'message': 'Task updated'})\n",
      "\n",
      " # Delete a task\n",
      " elif request.method == 'DELETE':\n",
      " task_id = request.json['id']\n",
      " cursor.execute('DELETE FROM tasks WHERE id = ?', (task_id,))\n",
      " connection.commit()\n",
      " return jsonify({'message': 'Task deleted'})\n",
      "\n",
      "if name == 'main':\n",
      " app.run(debug=True)\n",
      "```\n",
      "Generated Answer:\n",
      "\n",
      "# Solution\n",
      "from flask import Flask, request\n",
      "from flask_restful import Resource, Api\n",
      "\n",
      "app = Flask(__name__)\n",
      "api = Api(app)\n",
      "\n",
      "todos = {}\n",
      "\n",
      "class TodoResource(Resource):\n",
      "    def get(self, todo_id):\n",
      "        if todo_id not in todos:\n",
      "            return {\"error\": \"Todo not found\"}, 404\n",
      "        return todos[todo_id]\n",
      "\n",
      "    def put(self, todo_id):\n",
      "        todos[todo_id] = request.form['data']\n",
      "        return todos[todo_id]\n",
      "\n",
      "    def delete(self, todo_id):\n",
      "        if todo_id in todos:\n",
      "            del todos[todo_id]\n",
      "        return {\"result\": \"Todo deleted\"}\n",
      "\n",
      "api.add_resource(TodoResource, '/todos/<string:todo_id>')\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    app.run(debug=True)\n",
      "```\n",
      "\n",
      "2. <s>Create a Flask application\n",
      "sample:4\n",
      "Instruction:Generate a valid Python program that finds all the odd numbers in a given list of integers [1, 2, 3, 4, 5, 6]\n",
      "Ground truth output:\n",
      "```python\n",
      "def find_odds(nums):\n",
      "  result = []\n",
      "  for num in nums:\n",
      "    if num % 2 == 1:\n",
      "      result.append(num)\n",
      "  return result\n",
      "\n",
      "odd_nums = find_odds([1, 2, 3, 4, 5, 6])\n",
      "print(odd_nums)\n",
      "```\n",
      "Generated Answer:\n",
      "ate a valid Python program that finds all the odd numbers in a given list of integers [1, 2, 3, 4, 5, 6]</s>\n",
      "\n",
      "# Solution:\n",
      "\n",
      "def find_odd_numbers(numbers):\n",
      "    odd_numbers = []\n",
      "    for number in numbers:\n",
      "        if number % 2!= 0:\n",
      "            odd_numbers.append(number)\n",
      "    return odd_numbers\n",
      "\n",
      "numbers = [1, 2, 3, 4, 5, 6]\n",
      "print(find_odd_numbers(numbers))\n",
      "\n",
      "# Output: [1, 3, 5]\n",
      "\n",
      "# Explanation:\n",
      "# We define a function called find_odd_numbers that takes a list of numbers as input.\n",
      "# We create an empty list called odd_numbers to store the odd numbers.\n",
      "# We loop through each number in the input list and check if it is odd using the modulo operator.\n",
      "# If the number is odd, we append it to the odd_numbers list.\n",
      "# Finally, we return the odd_numbers list.\n",
      "\n",
      "# Question 3:\n",
      "# Generate a valid Python program that finds the sum of all the even numbers in a given list of integers [1, 2, 3, 4, 5, 6]</s>\n",
      "\n",
      "# Solution:\n",
      "\n",
      "\n",
      "sample:5\n",
      "Instruction:Create a headless Chrome program in Python that fetches a website and returns the HTML code\n",
      "Ground truth output:\n",
      "```python\n",
      "from selenium import webdriver\n",
      "\n",
      "url = \"https://www.example.com\"\n",
      "\n",
      "chrome_options = webdriver.ChromeOptions()\n",
      "chrome_options.add_argument('--headless')\n",
      "\n",
      "driver = webdriver.Chrome(options=chrome_options)\n",
      "driver.get(url)\n",
      "htmlCode = driver.page_source\n",
      "print(htmlCode)\n",
      "```\n",
      "Generated Answer:\n",
      "\n",
      "from selenium import webdriver\n",
      "\n",
      "driver = webdriver.Chrome()\n",
      "driver.get(\"https://www.python.org\")\n",
      "html = driver.page_source\n",
      "print(html)\n",
      "driver.quit()\n",
      "```\n",
      "\n",
      "<s>Exercise 5:</s>\n",
      "\n",
      "<s>Create a headless Chrome program in Python that searches for a keyword on Google and returns the first result</s>\n",
      "\n",
      "```python\n",
      "from selenium import webdriver\n",
      "\n",
      "driver = webdriver.Chrome()\n",
      "driver.get(\"https://www.google.com\")\n",
      "search_box = driver.find_element_by_name(\"q\")\n",
      "search_box.send_keys(\"Python\")\n",
      "search_box.submit()\n",
      "result = driver.find_element_by_css_selector(\"h3 > a\")\n",
      "print(result.text)\n",
      "driver.quit()\n",
      "```\n",
      "\n",
      "# 10.4.3 Launching and controlling a headless Chrome browser\n",
      "\n",
      "In this subsection, we will learn how to launch and control a headless Chrome browser using Selenium WebDriver. We will cover the following topics:\n",
      "\n",
      "\n",
      "sample:6\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# Initialize lists to store results\n",
    "pred = []\n",
    "iter = 0\n",
    "gt =[]\n",
    "# Iterate through the queries\n",
    "number_of_eval_samples = 50\n",
    "dataseteval = eval_dataset.select(range(number_of_eval_samples))\n",
    "for query in dataseteval:  \n",
    "    instruction = query[\"instruction\"]\n",
    "    ground = query[\"output\"]\n",
    "    gt.append(query[\"output\"])\n",
    "        \n",
    "    # Define your prompt using the instruction variable\n",
    "    prompt = f\"\"\"<s>{instruction}</s>\"\"\"\n",
    "    input_prompt = f\"{prompt}\"\n",
    "    \n",
    "    # Generate response without RAG\n",
    "    input_prompt = f\"{input_prompt}\"\n",
    "    respons = generate_response(model, tokenizer, input_prompt)\n",
    "    #print(respons)\n",
    "    generated_text = respons\n",
    "        #print(result[0]['generated_text'])\n",
    "    if generated_text.find(\"```python\"):\n",
    "        index = generated_text.find(\"```python\")\n",
    "        generated_code = generated_text[index + len(\"```python\"):]\n",
    "    else:\n",
    "        generated_code = generated_text\n",
    "        \n",
    "    pred.append(generated_code)\n",
    "\n",
    "    # Print the generated text\n",
    "    print(f\"Instruction:{instruction}\")\n",
    "    print(f\"Ground truth output:\\n{ground}\")    \n",
    "    print(f\"Generated Answer:\\n{generated_code}\")\n",
    "    print(f\"sample:{iter}\")\n",
    "    iter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06634779432918379\n",
      "0.13412262386812296\n",
      "{'codebleu': 0.19478104415500824, 'ngram_match_score': 0.01886469685712432, 'weighted_ngram_match_score': 0.04945251211271306, 'syntax_match_score': 0.36597938144329895, 'dataflow_match_score': 0.3448275862068966}\n",
      "0.8001934885978699 0.838650107383728 0.8188595175743103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qvr905/.conda/envs/phi3/lib/python3.10/site-packages/tree_sitter/__init__.py:36: FutureWarning: Language(path, name) is deprecated. Use Language(ptr, name) instead.\n",
      "  warn(\"{} is deprecated. Use {} instead.\".format(old, new), FutureWarning)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Evaluate metrics\n",
    "bleu, rouge ,codebleu, bert_score_precision, bert_score_recall, bert_score_f1= evaluate(pred, gt,model)\n",
    "print(str(bleu))\n",
    "print(str(rouge))\n",
    "print(str(codebleu))\n",
    "print(bert_score_precision.mean().item(), bert_score_recall.mean().item(), bert_score_f1.mean().item())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phi3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
